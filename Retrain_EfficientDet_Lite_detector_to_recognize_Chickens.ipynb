{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Retrain EfficientDet-Lite detector to recognize Chickens",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "license"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zebular13/Ambassador_Agreement/blob/master/Retrain_EfficientDet_Lite_detector_to_recognize_Chickens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "license"
      },
      "source": [
        "##### *Copyright 2021 Google LLC*\n",
        "*Licensed under the Apache License, Version 2.0 (the \"License\")*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "rKwqeqWBXANA"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Retrain EfficientDet for the Edge TPU with TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "In this tutorial, we'll retrain the EfficientDet-Lite object detection model (derived from [EfficientDet](https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html)) using the [TensorFlow Lite Model Maker library](https://www.tensorflow.org/lite/guide/model_maker).\n",
        "\n",
        "We'll retrain the model using a publicly available dataset of chicken photos, teaching the model to recognize a chicken by name.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzoNZRp4sVxK"
      },
      "source": [
        "To start running all the code in this tutorial, select **Runtime > Run all** in the Colab toolbar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "## Import the required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74DzzUU3MNi3"
      },
      "source": [
        "Import [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker). \n",
        "We'll also be using numpy and os packages. \n",
        "Finally, i mount google drive (this is where my images are stored). It will provide a link that you must follow to sign into google drive. Copy the code here and paste it back into the field that will show up in this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "source": [
        "!pip install -q tflite-model-maker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0XM-oIfhgQ7"
      },
      "source": [
        "## Load the training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRd13bfetO7B"
      },
      "source": [
        "The chicken dataset includes labels to identify four individual chickens:\n",
        "\"Angel\", \"Broccoli\", \"Angel Baby\" and \"LadyFeathers MgGee\".\n",
        "\n",
        "Model Maker requires that we load our dataset using the [`DataLoader`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/DataLoader) API. So in this case, we'll load it from a CSV file in google drive that defines 2384 images for training, 300 images for validation, and 300 images for testing. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ObtdneqvP5"
      },
      "source": [
        "train_data, validation_data, test_data = object_detector.DataLoader.from_csv('gdrive/MyDrive/chickens3/chickens-8.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W0Njyxoq0gH"
      },
      "source": [
        "The CSV is in this format:\n",
        "\n",
        "|| Asssigned Set | Image path | Label | xmin | ymin  | xmax | ymax|\n",
        "|-|--------------|-----------|---------------|--------|-------|------|------|\n",
        "||  TRAIN        | gdrive/MyDrive/chickens3/chickens2-mp4-2565.jpg       | Broccoli        | 487      | 3 | 636 | 350 |\n",
        "||  TRAIN        | gdrive/MyDrive/chickens3/chickens2-mp4-2565.jpg       | Ladyfeathers MgGee   | 428    | 25 |  545 | 476 |\n",
        "||  TRAIN   | gdrive/MyDrive/chickens3/chickens2-mp4-2565.jpg       | Angel          | 296               |15  | 636| 350|\n",
        "||  VALIDATION         | gdrive/MyDrive/chickens3/chickens2-mp4-2567.jpg       | Broccoli           | 487               | 3 | 636 | 350|\n",
        "||  TEST         | gdrive/MyDrive/chickens3/chickens2-mp4-2570.jpg       | Broccoli           | 486               | 1| 631 | 349 |\n",
        "||  TEST         | gdrive/MyDrive/chickens3/chickens2-mp4-2570.jpg       | Angel Baby           | 189               | 347  | 543| 476 |\n",
        "||\n",
        "\n",
        "If you want to load your own dataset as a CSV file, you can learn more about the format in [Formatting a training data CSV](https://cloud.google.com/vision/automl/object-detection/docs/csv-format). You can load your CSV either from [Cloud Storage](https://cloud.google.com/storage) (as shown above) or from a local path.\n",
        "\n",
        "[`DataLoader`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/DataLoader) can also load your dataset in other formats, such as from a set of TFRecord files or from a local directory using the PASCAL VOC format. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8clx0KPutCM"
      },
      "source": [
        "## Select the model spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn61LJ9QbOPi"
      },
      "source": [
        "Model Maker supports the EfficientDet-Lite family of object detection models. (EfficientDet-Lite is derived from [EfficientDet](https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html), which offer state-of-the-art accuracy in a small model size). There are several model sizes you can choose from:\n",
        "\n",
        "|| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|-|--------------------|-----------|---------------|----------------------|\n",
        "|| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
        "|| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
        "|| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
        "|| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
        "|| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
        "| <td colspan=4><br><i>* File size of the integer quantized models. <br/>** Latency measured on Pixel 4 using 4 threads on CPU. <br/>*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.</i></td> |\n",
        "\n",
        "Beware that the bigger models (Lite3 and Lite4) do not fit onto the Edge TPU's onboard memory, so you'll see even greater latency when using those due to the cost of fetching data from the host system memory. Maybe this extra latency is okay for your application, but if it's not and you require the precision of the larger models, then you can [pipeline the model across multiple Edge TPUs](https://coral.ai/docs/edgetpu/pipeline/) (more about this when we compile the model below).\n",
        "\n",
        "For this tutorial, we'll use Lite0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM9gePHw9Jv1"
      },
      "source": [
        "spec = object_detector.EfficientDetLite4Spec()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnCzdzs0-Rbo"
      },
      "source": [
        "The [`EfficientDetLite0Spec`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/EfficientDetLite0Spec) constructor also supports several arguments that specify training options, such as the max number of detections (default is 25 for the TF Lite model) and whether to use Cloud TPUs for training. You can also use the constructor to specify the number of training epochs and the batch size, but you can also specify those in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qjq2UEHCLUi"
      },
      "source": [
        "## Create and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "Now we need to create our model according to the model spec, load our dataset into the model, specify training parameters, and begin training. \n",
        "\n",
        "Using Model Maker, we accomplished all of that with [`create()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/create):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "source": [
        "model = object_detector.create(train_data=train_data, \n",
        "                               model_spec=spec, \n",
        "                               validation_data=validation_data, \n",
        "                               epochs=50, \n",
        "                               batch_size=32, \n",
        "                               train_whole_model=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n5-o3vvGfnJ"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "Now we'll use the remaining 300 images in our test dataset to evaluate how well the model performs with data it has never seen before.\n",
        "\n",
        "The default batch size for [EfficientDetLite models](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/EfficientDetSpec) is 64. To make sure it can go through all images in one step, you can specify the `batch_size` argument to 300 when you call [`evaluate()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/ObjectDetector#evaluate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "source": [
        "model.evaluate(test_data,\n",
        "               batch_size=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl9VmwtNP6yd"
      },
      "source": [
        "## What do these metrics mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEon9xd2BDS_"
      },
      "source": [
        "The [`evaluate()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/ObjectDetector#evaluate) method provides output in the style of [COCO evaluation metrics](https://cocodataset.org/#detection-eval):\n",
        "\n",
        "\n",
        "***Average Precision:***\n",
        "* 'AP':  Average Precision \n",
        "* 'AP50': AP at IoU=.50 (PASCAL VOC metric)\n",
        "* 'AP75': AP at IoU=.75 (strict metric)\n",
        "\n",
        "***Average Precision for each Label:***\n",
        "* 'AP_/Label_x': \n",
        "\n",
        "***Average Precision Across Scales:***\n",
        "* 'APl': AP for large objects: area > 96 ** 2\n",
        "* 'APm': AP for medium objects: 322 < area < 96 ** 2\n",
        "* 'APs':  AP for small objects: area < 32 ** 2\n",
        "\n",
        "***Average Recall***\n",
        "* 'ARl': \n",
        "* 'ARm': \n",
        "* 'ARmax1': \n",
        "* 'ARmax10':\n",
        "* 'ARmax100': \n",
        "* 'ARs': \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yB_XMpqGlLs"
      },
      "source": [
        "## Export to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "Next, we'll export the model to the TensorFlow Lite format. By default, the [`export()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/ObjectDetector#export) method performs [full integer post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization), which is exactly what we need for compatibility with the Edge TPU. (Model Maker uses the same dataset we gave to our model spec as a representative dataset, which is required for full-int quantization.)\n",
        "\n",
        "We just need to specify the export directory and format. By default, it exports to TF Lite, but we also want a labels file, so we declare both:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKd6qk7TbxYO"
      },
      "source": [
        "model.export(export_dir='.',\n",
        "             tflite_filename='efficientdet-lite-chickens.tflite',\n",
        "             label_filename='chicken-labels.txt',\n",
        "             export_format=[ExportFormat.TFLITE, ExportFormat.LABEL])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94hZ-exOCRB"
      },
      "source": [
        "### Evaluate the TF Lite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQpahAIBqBPp"
      },
      "source": [
        "Exporting the model to TensorFlow Lite can affect the model accuracy, due to the reduced numerical precision from quantization and because the original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TF Lite model uses global NMS, which is faster but less accurate.\n",
        "\n",
        "Therefore you should always evaluate the exported TF Lite model and be sure it still meets your requirements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS3Ell_lqH4e"
      },
      "source": [
        "model.evaluate_tflite('efficientdet-lite-chickens.tflite', test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph88z7PdOeX7"
      },
      "source": [
        "### Test it on a new image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6_RwPZqNhX"
      },
      "source": [
        "Just to be sure of things, let's run an inference with the TF Lite model ourselves. \n",
        "\n",
        "To simplify our code, we'll use the [PyCoral API](https://coral.ai/docs/reference/py/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgtGBqua1N3"
      },
      "source": [
        "! python3 -m pip install --extra-index-url https://google-coral.github.io/py-repo/ pycoral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkXtipXKqXp4"
      },
      "source": [
        "INPUT_IMAGE = 'gdrive/MyDrive/chickens3/chickens2-mp4-47367.jpg'\n",
        "#INPUT_IMAGE = 'gdrive/MyDrive/chickenspics/chickens-5549.jpeg'\n",
        "\n",
        "# Set the model files\n",
        "MODEL_FILE = 'efficientdet-lite-chickens.tflite'\n",
        "LABELS_FILE = 'chicken-labels.txt'\n",
        "DETECTION_THRESHOLD = 0.1\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "\n",
        "import tflite_runtime.interpreter as tflite \n",
        "from pycoral.adapters import common\n",
        "from pycoral.adapters import detect\n",
        "from pycoral.utils.dataset import read_label_file\n",
        "\n",
        "def draw_objects(draw, objs, labels):\n",
        "  \"\"\"Draws the bounding box and label for each object.\"\"\"\n",
        "  COLORS = np.random.randint(0, 255, size=(len(labels), 3), dtype=np.uint8)\n",
        "  for obj in objs:\n",
        "    bbox = obj.bbox\n",
        "    color = tuple(int(c) for c in COLORS[obj.id])\n",
        "    draw.rectangle([(bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax)],\n",
        "                   outline=color, width=15)\n",
        "    font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", size=90)\n",
        "    draw.text((bbox.xmin + 20, bbox.ymin + 20),\n",
        "              '%s\\n%.2f' % (labels.get(obj.id, obj.id), obj.score),\n",
        "              fill=color, font=font)\n",
        "\n",
        "# Load the TF Lite model\n",
        "labels = read_label_file(LABELS_FILE)\n",
        "interpreter = tflite.Interpreter(MODEL_FILE)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Resize the image\n",
        "image = Image.open(INPUT_IMAGE)\n",
        "_, scale = common.set_resized_input(\n",
        "    interpreter, image.size, lambda size: image.resize(size, Image.ANTIALIAS))\n",
        "\n",
        "# Run inference and draw boxes\n",
        "interpreter.invoke()\n",
        "objs = detect.get_objects(interpreter, DETECTION_THRESHOLD, scale)\n",
        "draw_objects(ImageDraw.Draw(image), objs, labels)\n",
        "\n",
        "# Show the results\n",
        "width = 400\n",
        "height_ratio = image.height / image.width\n",
        "image.resize((width, int(width * height_ratio)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyBBvyqx0XRn"
      },
      "source": [
        "## Download the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M43URVgg0ZcB"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('efficientdet-lite-chickens.tflite')\n",
        "files.download('efficientdet-lite-chickens_edgetpu.tflite')\n",
        "files.download('chicken-labels.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS4u77W5gnzQ"
      },
      "source": [
        "## More resources\n",
        "\n",
        "* For more information about the Model Maker library used in this tutorial, see the [TensorFlow Lite Model Maker guide](https://www.tensorflow.org/lite/guide/model_maker) and [API reference](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker).\n",
        "\n",
        "* For other transfer learning tutorials that are compatible with the Edge TPU, see the [Colab tutorials for Coral](https://github.com/google-coral/tutorials#colab-tutorials-for-coral).\n",
        "\n",
        "* You can also find more examples that show how to run inference on the Edge TPU at [coral.ai/examples](https://coral.ai/examples/#code-examples/)."
      ]
    }
  ]
}